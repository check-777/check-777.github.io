<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>树模型总结</title>
      <link href="/2022/05/29/%E6%A0%91%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/"/>
      <url>/2022/05/29/%E6%A0%91%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h2 id="树模型"><a href="#树模型" class="headerlink" title="树模型"></a>树模型</h2><h2 id="谈谈你对熵、信息增益和信息增益比的理解？"><a href="#谈谈你对熵、信息增益和信息增益比的理解？" class="headerlink" title="谈谈你对熵、信息增益和信息增益比的理解？"></a>谈谈你对熵、信息增益和信息增益比的理解？</h2><p>首先要明白，这里涉及的熵，信息增益和信息增益比都是决策树进行特征选择(树根据特征选择分裂的一个值)。<br>1.熵:本质就是混沌度或者说不确定性的度量。不确定性越大,熵越大。<br>熵的计算公式为</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">H(p)=-\sum^n_1p_ilogp_i</span><br></pre></td></tr></table></figure><p><code>$log(p_i)$</code>可以看作是一种可能性的信息量，一个事件的总信息量其实就是每一种可能的信息量的数学期望。总结一句话，熵就是信息量的数据期望。</p><p>Note:<br>&lt;1&gt;对同一个随机变量，当概率分布为均匀分布的时候不确定性最大，熵也最大。<br>&lt;2&gt;对有相同概率分布的不同的随机变量，取值越多的随机变量熵越大。<br>Note:第一点比较难理解，第二点假设满足均匀分布,当均匀分布的范围越大，取值也就越多，因此熵越大。</p><p>2.条件熵:定义为已知随机变量X的条件下随机变量Y的不确定性。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">H(Y|X)=-\sum^n_1p_iH(Y|X=x_i)</span><br></pre></td></tr></table></figure><p>3.信息增益:信息增益是得知特征X的情况下而使得Y的信息不确定性减少的程度。信息增益比是经验熵和条件熵的差值。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">g(A|D)=H(D)-H(D｜A)</span><br></pre></td></tr></table></figure><p>熵和条件熵之差被称为互信息，可见决策树根据信息增益进行特征选择的时候，本质上就是增加类和特征之间的一个互信息。<br>Note:经验熵实际上就是熵，只不是实在极大似然估计下得到的。<br>(什么是极大似然估计)</p><p>4.信息增益比<br>信息增益比定义为信息增益和训练数据集D关于特征A的熵之比。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">g_R(D,A) = g(A|D)/H_A(D)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">H_A(D) = -\sum_&#123;i=1&#125;^n\frac&#123;|D_&#123;i&#125;|&#125;&#123;D&#125;log_2\frac&#123;|D_&#123;i&#125;|&#125;&#123;D&#125;</span><br></pre></td></tr></table></figure><p>i为特征A的在整个数据集中的取值个数。&#x3D;&#x3D;<code>$H_A(D)$</code>和类别完全无关，只根据特征自身的分布进行计算。&#x3D;&#x3D;<br>信息增益比相比于信息增益的好处:<br>信息增益会偏向于选择特征取值较多的特征，举一个极端点的例子，比如使用样本的ID作为其特征，这样的话条件熵为0。因此信息增益全部为经验熵，因此信息增益比最大。但是以ID当作特征划分是毫无意义的。<br>而对于信息增益比，如果特征的取值很多的时候,特征的熵<code>$H_A(D)$</code>也会变大，则整个信息增益比变小。因此起到一个惩罚那些特征多的特征的信息增益的作用。</p><h2 id="ID3算法的划分标准是什么？"><a href="#ID3算法的划分标准是什么？" class="headerlink" title="ID3算法的划分标准是什么？"></a>ID3算法的划分标准是什么？</h2><p>首先ID3使用的是信息增益来进行划分,表示得知特征A的信息而使得样本集合不确定性减少的程度(特征和类别的之间的互信息最大)。</p><ol><li>针对某个数据集的信息熵:</li></ol><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">H(D) = -\sum_&#123;k=1&#125;^&#123;K&#125;\frac&#123;C_&#123;k&#125;&#125;&#123;D&#125;log2\frac&#123;C_&#123;k&#125;&#125;&#123;D&#125;</span><br></pre></td></tr></table></figure><p>其中<code>$C_&#123;k&#125;$</code>表示第k个类别的样本子集个数,D是数据集的个数,K是类别的个数</p><ol start="2"><li>针对特征A的条件熵</li></ol><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">H(D|A) = \sum_&#123;i=1&#125;^&#123;n&#125;\frac&#123;|D_&#123;i&#125;|&#125;&#123;D&#125;H(D_&#123;i&#125;) = -\sum_&#123;i=1&#125;^&#123;n&#125;\frac&#123;|D_&#123;i&#125;|&#125;&#123;D&#125;H(D_&#123;i&#125;)(\sum_&#123;k=1&#125;^&#123;K&#125;\frac&#123;|D_&#123;ik&#125;|&#125;&#123;|Di|&#125;log2\frac&#123;|D_&#123;ik&#125;|&#125;&#123;|D_&#123;i&#125;|&#125;)</span><br></pre></td></tr></table></figure><p>其中<code>$D_&#123;i&#125;$</code>是对于特征A取第i个值的样本子,<code>$D_&#123;ik&#125;$</code>表示<code>$D_&#123;i&#125;$</code>中属于第k类的样本子集。<br>3. 信息增益</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Gain(D,A) = H(D) - H(D|A)</span><br></pre></td></tr></table></figure><p>&#x3D;&#x3D;信息增益越大,表示特征A用来划分树的纯度提升越大,特征和类别之间的互信息越大以及特征A使得样本不确定性的减少的程度越大。&#x3D;&#x3D;</p><h2 id="C4-5的划分标准是什么？"><a href="#C4-5的划分标准是什么？" class="headerlink" title="C4.5的划分标准是什么？"></a>C4.5的划分标准是什么？</h2><p>信息增益比</p><h2 id="C4-5是如何处理缺失值的？"><a href="#C4-5是如何处理缺失值的？" class="headerlink" title="C4.5是如何处理缺失值的？"></a>C4.5是如何处理缺失值的？</h2><p>个人理解这是一种常见的数据处理策略</p><p>1.在特征值缺失的情况下进行划分特征的选择？即如何计算特征的信息增益率<br>计算没有缺失的特征值的均值进行填充,再进行划分<br>2.选定该划分特征，对于缺失该特征值的样本如何处理？即到底把这个样本划分到哪个结点里<br>直接将缺失该特征值的样本以不同的概率(分配权重)分配到子节点当</p><h2 id="ID3算法有什么缺陷？C4-5算法是如何解决ID3的缺陷的？ID3和C4-5存在什么缺陷？"><a href="#ID3算法有什么缺陷？C4-5算法是如何解决ID3的缺陷的？ID3和C4-5存在什么缺陷？" class="headerlink" title="ID3算法有什么缺陷？C4.5算法是如何解决ID3的缺陷的？ID3和C4.5存在什么缺陷？"></a>ID3算法有什么缺陷？C4.5算法是如何解决ID3的缺陷的？ID3和C4.5存在什么缺陷？</h2><p>ID3的缺陷:<br>1.只能用在离散特征，不能用在连续特征<br>2.没有剪枝策略，很容易过拟合<br>3.信息增益准则偏好取值较多的特征,比如ID编号，信息增益最大，因为条件熵为0。</p><p>C4.5如何解决的:<br>1.引入信息增益比,使得对取值较多的特征进行了惩罚。<br>2.引入悲观剪枝策略进行后剪枝缓解了过拟合问题。<br>3.对于连续特征，进行特征离散化操作，比如对于取值为m的连续特征，先进行一个连续值排序操作，然后取两点之间的均值作为m-1<br>个划分点。分别计算每个划分点作为二元分类的信息增益比，取信息增益比最大的划分点进行划分。</p><p>ID3和C4.5的缺陷:<br>对数运算和排序算法，很吃cpu<br>依然容易出现过拟合问题<br>++生成多叉树，不利于计算机计算++</p><h2 id="决策树剪枝有哪些策略？其优缺点分别是什么？"><a href="#决策树剪枝有哪些策略？其优缺点分别是什么？" class="headerlink" title="决策树剪枝有哪些策略？其优缺点分别是什么？"></a>决策树剪枝有哪些策略？其优缺点分别是什么？</h2><p>剪枝是防止决策树过拟合的方法。一棵完全生长的决策树很可能失去泛化能力，因此需要剪枝。</p><ul><li><p>剪枝的策略<br>剪枝分为预剪枝和后剪枝两种，预剪枝是在构建决策树时抑制它的生长，后剪枝是决策树生长完全后再对叶子节点进行修剪。</p></li><li><p>预剪枝策略<br>1.给树设置一个最大的高度和深度,或者一个最大的结点数目，超过这个数据则停止生长<br>2.为信息增益等评估准则设置阈值</p></li></ul><h2 id="CART是如何对连续值处理的？"><a href="#CART是如何对连续值处理的？" class="headerlink" title="CART是如何对连续值处理的？"></a>CART是如何对连续值处理的？</h2><p>对于回归问题。<br>1.Cart首先采用启发式的算法遍历所有的特征j和其取值s，其中j作为切分变量,s作为切分点。通过优化MSE来选择最优切分变量和切分点(切分变量就是选择特征,切分点就是选择该特征下的特征值)</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\underset&#123;j,s&#125;&#123;min&#125;\left [ \underset&#123;c1&#125;&#123;min&#125;\sum_&#123;x_&#123;i&#125; \in R_&#123;1&#125;(j,s)&#125;(y_&#123;i&#125;-c_&#123;1&#125;)^2 +\underset&#123;c2&#125;&#123;min&#125;\sum_&#123;x_&#123;i&#125; \in R_&#123;1&#125;(j,s)&#125;(y_&#123;i&#125;-c_&#123;2&#125;)^2  \right ] </span><br></pre></td></tr></table></figure><p>其中c1和c2是划分后的两个子树的回归值，优化上式后，选择达到最小值的对(j,s)<br>2.选择相应的(j,s)并且决定输出值<br>然后划分两个区域:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">R_&#123;1&#125;(j,s) = \left \&#123; x|x^&#123;j&#125;\le s\right \&#125; 和R_&#123;2&#125;(j,s) = \left \&#123; x|x^&#123;j&#125; &gt; s\right \&#125;</span><br></pre></td></tr></table></figure><p>输出值为:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\hat&#123;c&#125;_&#123;m&#125; =\frac&#123;1&#125;&#123;N_&#123;m&#125;&#125; \sum_&#123;x_&#123;i&#125;\in R_&#123;m&#125;(j,s)&#125;y_&#123;i&#125;,x \in R_&#123;m&#125;, m=1,2</span><br></pre></td></tr></table></figure><p>其中<code>$N_&#123;m&#125;$</code>是该子结点的所划分的样本个数,总体来说就是该子结点中所有样本真实值的均值。<br>具体推导见[<a href="https://www.bilibili.com/video/BV1mN411Z7j1?p=4%5D">https://www.bilibili.com/video/BV1mN411Z7j1?p=4]</a><br>3.递归的重复(1)(2)<br>4.生成决策树<br>输出空间被划分为M个区域，每个区域对应一个<code>$\hat&#123;c&#125;_&#123;m&#125;$</code></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">f(x)=\sum_&#123;m=1&#125;^&#123;M&#125;\hat&#123;c&#125;_&#123;m&#125;I(x \in R_&#123;m&#125;)</span><br></pre></td></tr></table></figure><h2 id="CART的剪枝策略是什么？"><a href="#CART的剪枝策略是什么？" class="headerlink" title="CART的剪枝策略是什么？"></a>CART的剪枝策略是什么？</h2><p><a href="https://note.youdao.com/">https://zhuanlan.zhihu.com/p/145215188</a><br>下次整理</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 树模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/05/29/hello-world/"/>
      <url>/2022/05/29/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
